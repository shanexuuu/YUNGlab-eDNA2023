#00 Coral ITS2 cut primer

##----------ITS2-------------------
	mkdir -p 02_ITS2_noprimer
	
	cp 01_demultiplex/I*/*.gz 02_ITS2_noprimer/

	# R remove primer
	conda activate qiime2-amplicon-2024.2 

	R
#### DADA2 analysis of fastq files
#### Paired-end 
#--------------- DADA2 processing ---------------#
# Load library and functions
	library(dada2); packageVersion("dada2") #1.31.0
	library(ShortRead); packageVersion("ShortRead") #1.60.0
	library(tidyverse); packageVersion("tidyverse") #2.0.0
	library(Biostrings); packageVersion("Biostrings") # 2.70.3
# Load sequence reads

	output_folder <- "02_ITS2_noprimer"

	path <- "02_ITS2_noprimer"

	fnFs <- sort(list.files(path, pattern="_R1_001.fastq.gz", full.names = T)) # Forward read files
	fnRs <- sort(list.files(path, pattern="_R2_001.fastq.gz", full.names = T)) # Reverse read files
# Get sample names, assuming files named as so: SAMPLENAME_XXX.fastq
	sample_names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
	View(sample_names) 
# Visualize quality
	plotQualityProfile(fnFs[1])
	plotQualityProfile(fnRs[1])
# ------------------------ Primer removal check ---------------------------- #
# Identify primers
FWD <- "GARTCTTTGAACGCAAATGGC" # Coral_ITS2_F
REV <- "GCTTATTAATATGCTTAAATTCAGCG" # Coral_ITS2_R

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

FWD_orients <- allOrients(FWD)
REV_orients <- allOrients(REV)

# Identify primers
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

seq_id <- 1
rbind(FWD.ForwardReads = sapply(FWD_orients, primerHits, fn = fnFs[[seq_id]]),
      FWD.ReverseReads = sapply(FWD_orients, primerHits, fn = fnRs[[seq_id]]), 
      REV.ForwardReads = sapply(REV_orients, primerHits, fn = fnFs[[seq_id]]), 
      REV.ReverseReads = sapply(REV_orients, primerHits, fn = fnRs[[seq_id]]))

# Remove Primer
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

	cutadapt <- "/usr/bin/cutadapt" # CHANGE ME to the cutadapt path on your machine
	system2(cutadapt, args = "--version") # Run shell commands from R

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

## check after primer cut
rbind(FWD.ForwardReads = sapply(FWD_orients, primerHits, fn = fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD_orients,
    primerHits, fn = fnRs.cut[[1]]), REV.ForwardReads = sapply(REV_orients, primerHits,
    fn = fnFs.cut[[1]]), REV.ReverseReads = sapply(REV_orients, primerHits, fn = fnRs.cut[[1]]))

	# Success! Primers are no longer detected in the cutadapted reads.

# Save no primer seq

	# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_R1_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_R2_001.fastq.gz", full.names = TRUE))

	# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]

sample.names <- unname(sapply(cutFs, get.sample.name))

View(sample.names)


# Performing filtering and trimming after primer removed!
filt_path <- file.path(path, "filtered_after_cut_primer") # Place filtered_after_cut_primer files in filtered/ subdirectory
filtFs <- file.path(filt_path, paste0(sample_names, "_F_no_primer_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample_names, "_R_no_primer_filt.fastq.gz"))


# Check quality after primer cut
	plotQualityProfile(cutFs[1:2])
	plotQualityProfile(cutRs[1:2])

## truncLen=c(forward,reverse) is based on quality result
## ITS has different read length and thus no need trunc end of reads
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs,
                     minLen = 100, # Remove unexpectedly short reads
                     maxN = 0, maxEE = c(2,2), truncQ = 2, rm.phix = T,
                     compress=TRUE, multithread=TRUE)
head(out)

	plotQualityProfile(filtFs[1])
	plotQualityProfile(filtRs[1])

# Exclude 0 seq samples, rename filtFs and filtRs
if(length(sample_names[out[,2]<1 | out[,1]<1]) > 0){
  filtFs <- file.path(filt_path, paste0(sample_names[out[,2]>0 & out[,1]>0], "_F_filt.fastq.gz"))
  filtRs <- file.path(filt_path, paste0(sample_names[out[,2]>0 & out[,1]>0], "_R_filt.fastq.gz"))
}

write.csv(out, paste0(output_folder, "/out_track.csv"))

q()
n

####········R cut primer finish·······································

#01 Prepare sequence without primers
- use filt.N_noprimer/

	cd 02_Coral_ITS2/
	nano sample_name.txt  #make sample_name.txt (a list of smaple names)
	mkdir -p 00_noprimer

	while read p;      
	    do     
	    cp ITS2_noprimer/filtered_after_cut_primer/''"$p"'_F_no_primer_filt.fastq.gz' 00_noprimer/''"$p"'_5_R1.fastq.gz';
	done < sample_name.txt

	while read p;      
	    do     
	    cp ITS2_noprimer/filtered_after_cut_primer/''"$p"'_R_no_primer_filt.fastq.gz' 00_noprimer/''"$p"'_5_R2.fastq.gz';
	done < sample_name.txt
	
	seqkit stat 00_noprimer/*R1.fastq.gz > 00_noprimer_stat.txt

#2 Run sequence read file through quality trimming: running window of Q25, 10-nt wide, mean Q25

	mkdir -p 02_trimmed

	while read p; 
	do
	 sickle pe  -f 00_noprimer/''"$p"'_R1.fastq.gz'  -r 00_noprimer/''"$p"'_R2.fastq.gz'  -t sanger  -o 02_trimmed/''"$p"'_R1_paired.fastq'  -p 02_trimmed/''"$p"'_R2_paired.fastq' -s 02_trimmed/''"$p"'_nonparied.fastq' -q 25;
	done < sample_name.txt

	seqkit stat 02_trimmed/*R1_paired.fastq > 02_trimmed_stat.txt

#3 usearch merge Mifish min 200 bp minovlen20 bp 
	mkdir -p 03_usearch_merge
## min200 
	while read p; 
	do
	 usearch -fastq_mergepairs 02_trimmed/''"$p"'_R1_paired.fastq'  -reverse 02_trimmed/''"$p"'_R2_paired.fastq'  -fastq_pctid 95  -fastqout 03_usearch_merge/''"$p"'_merge.fastq'  -fastq_minovlen 20 -fastq_minmergelen 200;
	done < sample_name.txt

	seqkit stat 03_usearch_merge/*fastq > 03_usearch_merge_stat.txt

#4 Quality-filtering merged reads * 0 mismatches, fastq_maxee_rate 0.005
	mkdir -p 04_quality_filt

	while read p; 
	do
         usearch -fastq_filter 03_usearch_merge/''"$p"'_merge.fastq'  -fastq_maxee_rate 0.005  -fastaout 04_quality_filt/''"$p"'_merge.filt.fasta';
	done < sample_name.txt

	seqkit stat 04_quality_filt/*_merge.filt.fasta > 04_quality_filt_stat.txt

#5 Add barcode labels onto sequence read identifiers
	mkdir -p 05_final_label

	while read p; 
	do
        python /media/SharedFolder/Script/fasta_number2.py 04_quality_filt/''"$p"'_merge.filt.fasta' "$p" > 05_final_label/''"$p"'_label.fasta'
	done < sample_name.txt

#6 Concatenate seq
	cat 05_final_label/*_label.fasta > cat_labeled.fa

#7 Dereplicate the sequences using usearch (64 bit), min= 2 reads  

	usearch -fastx_uniques cat_labeled.fa -fastaout cat_label_derep.fasta -sizeout 

#8 ZOTU (Unoise3 Denoising) 
	usearch  -unoise3 cat_label_derep.fasta -zotus cat_label_derep_zotus.fasta 

#9 Assign reads to ZOTUs
	usearch -otutab cat_labeled.fa -zotus cat_label_derep_zotus.fasta -otutabout cat_label_derep_zotus_otutab.txt -threads 180

## vsearch  Map reads back to centriods make ZOTU  with id 0.985
	vsearch-2.29.0/bin/vsearch --usearch_global cat_labeled.fa --db cat_label_derep_zotus.fasta --id 0.985 --otutabout cat_label_derep_zotus_otutab.txt --biomout cat_label_derep_zotus_otutab.biom  --threads 180 --log zotutab_log.txt

#10 LULU OTU  (post-curation)
- https://github.com/tobiasgf/lulu
	conda activate R_amplicon
## 10.1 Use blastn/vsearch global to compare dissimilarity
	makeblastdb -in cat_label_derep_zotus.fasta -parse_seqids -dbtype nucl #blast

	blastn -db cat_label_derep_zotus.fasta -outfmt '6 qseqid sseqid pident' -out match_list.txt -qcov_hsp_perc 80 -perc_identity 84 -query cat_label_derep_zotus.fasta

##10.2 input data in R
	R
	library("lulu")

	otutab <- read.csv("cat_label_derep_zotus_otutab.txt",sep='\t',header=TRUE,as.is=TRUE, row.names = 1)

	row_sums <- rowSums(otutab)

	non_zero_rows <- which(row_sums != 0)

	otutab_filtered <- otutab[non_zero_rows, ]

	matchlist <- read.table("match_list.txt", header=FALSE,as.is=TRUE, stringsAsFactors=FALSE)

##10.3 run LULU

	curated_result <- lulu(otutab_filtered, matchlist)

	# ....Which is equivalent of running LULU with default settings for the options minimum_ratio_type, minimum_ratio, minimum_relative_cooccurence  

	curated_result <- lulu(otutab_filtered, matchlist, minimum_ratio_type = "min", minimum_ratio = 1, minimum_match = 84, minimum_relative_cooccurence = 0.95)

	## Number of OTUs retained  
	curated_result$curated_count

	## Number of OTUs discarded 
	curated_result$discarded_count

	##save result
	curated_result$curated_table

	a <- data.frame(LULUOTU=rownames(curated_result$curated_table), curated_result$curated_table)  

	write.table(a, "curated_otutab.csv", row.names=F, col.names = T, sep=",")

	q()
	n

##10.4 extract luluotu.fasta

	awk 'BEGIN{ FS=",";OFS="\t" }{ print $1}' curated_otutab.csv > luluotuID.txt

	sed -i '1d' luluotuID.txt

	sed 's/^["]*//g' luluotuID.txt > luluotuID2.txt


	sed 's/["]*$//g' luluotuID2.txt >luluotuID_final.txt

	wc -l luluotuID_final.txt

	#seqkit extract
	seqkit grep -f luluotuID_final.txt cat_label_derep_zotus.fasta > luluotu_seq.fa

#11 Claident for taxonomy classification 
- https://github.com/astanabe/ClaidentTutorial
- Claident v8.24
#----------- Taxa assignment using claident -----------#

	FASTA_FILE="luluotu_seq.fa"

	OUTPUT_FOLDER="06_TaxaAssignmentOut"

	mkdir -p ${OUTPUT_FOLDER}

	# Check overall_class 
	conda activate qiime2-amplicon-2024.2

	Claident_v24Aug24/bin/clmakecachedb --blastdb=/SSD7TB_data/Claident/share/claident/blastdb/overall_class --numthreads=150 ${FASTA_FILE} overall_class_cache

	Claident_v24Aug24/bin/clidentseq --blastdb=/SSD7TB_data/Claident/share/claident/blastdb/overall_class --numthreads=50 ${FASTA_FILE} overall_class_clidentseq

	Claident_v24Aug24/bin/classigntax --taxdb=/SSD7TB_data/Claident/share/claident/taxdb/overall_class --maxpopposer=0.05 --minsoratio=19 overall_class_clidentseq overall_class_classigntax

	# Overall_genus
	Claident_v24Aug24/bin/clmakecachedb --blastdb=/SSD7TB_data/Claident/share/claident/blastdb/overall_genus --numthreads=150 ${FASTA_FILE} overall_genus_cache

	Claident_v24Aug24/bin/clidentseq --blastdb=/SSD7TB_data/Claident/share/claident/blastdb/overall_genus --numthreads=150 ${FASTA_FILE} overall_genus_clidentseq

	Claident_v24Aug24/bin/classigntax --taxdb=/SSD7TB_data/Claident/share/claident/taxdb/overall_genus --maxpopposer=0.05 --minsoratio=19 overall_genus_clidentseq overall_genus_classigntax

	# Merge identification results (overall_class + overall_genus)
	Claident_v0.9.2024.08.24/bin/clmergeassign --priority=equal --preferlower overall_genus_classigntax overall_class_classigntax merge_classigntax

	# Move file
	cp overall_class_clidentseq ./${OUTPUT_FOLDER}/

	cp overall_class_classigntax ./${OUTPUT_FOLDER}/

	cp overall_genus_clidentseq ./${OUTPUT_FOLDER}/

	cp overall_genus_classigntax ./${OUTPUT_FOLDER}/

	cp merge_classigntax ./${OUTPUT_FOLDER}/