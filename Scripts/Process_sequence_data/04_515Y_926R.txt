# 16S V4V5 region 515Y_926R
- use LULU otu in default setting
- Reference: Holman, Luke E., et al. "Animals, protists and bacteria share marine biogeographic patterns." Nature Ecology & Evolution 5.6 (2021): 738-746.

##00  R dada2 remove primer

	conda activate qiime2-amplicon-2024.2 

	R
#### DADA2 analysis of fastq files
#### Paired-end 
#--------------- DADA2 processing ---------------#
# Load library and functions
	library(dada2); packageVersion("dada2") #1.31.0
	library(ShortRead); packageVersion("ShortRead") #1.60.0
	library(tidyverse); packageVersion("tidyverse") #2.0.0
	library(Biostrings); packageVersion("Biostrings") # 2.70.3
# Load sequence reads

	output_folder <- "02_R_noprimer"

	path <- "01raw"

	fnFs <- sort(list.files(path, pattern=".raw_1.fastq.gz", full.names = T)) # Forward read files
	fnRs <- sort(list.files(path, pattern=".raw_2.fastq.gz", full.names = T)) # Reverse read files
# Get sample names, assuming files named as so: SAMPLENAME_XXX.fastq
	sample_names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
	View(sample_names) 

# ------------------------ Primer removal check ---------------------------- #
# Identify primers
FWD <- "GTGYCAGCMGCCGCGGTAA" # 515Y
REV <- "CCGYCAATTYMTTTRAGTTT" # 926R

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

FWD_orients <- allOrients(FWD)
REV_orients <- allOrients(REV)

# Identify primers
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

seq_id <- 1
rbind(FWD.ForwardReads = sapply(FWD_orients, primerHits, fn = fnFs[[seq_id]]),
      FWD.ReverseReads = sapply(FWD_orients, primerHits, fn = fnRs[[seq_id]]), 
      REV.ForwardReads = sapply(REV_orients, primerHits, fn = fnFs[[seq_id]]), 
      REV.ReverseReads = sapply(REV_orients, primerHits, fn = fnRs[[seq_id]]))

# Remove Primer
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

	cutadapt <- "/usr/bin/cutadapt" # CHANGE ME to the cutadapt path on your machine
	system2(cutadapt, args = "--version") # Run shell commands from R

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

## check after primer cut
rbind(FWD.ForwardReads = sapply(FWD_orients, primerHits, fn = fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD_orients,
    primerHits, fn = fnRs.cut[[1]]), REV.ForwardReads = sapply(REV_orients, primerHits,
    fn = fnFs.cut[[1]]), REV.ReverseReads = sapply(REV_orients, primerHits, fn = fnRs.cut[[1]]))

	# Success! Primers are no longer detected in the cutadapted reads.


# Save no primer seq
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = ".raw_1.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = ".raw_2.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]

sample.names <- unname(sapply(cutFs, get.sample.name))

head(sample.names)


# Performing filtering and trimming after primer removed!
filt_path <- file.path(path, "filtered_after_cut_primer") # Place filtered_after_cut_primer files in filtered/ subdirectory
filtFs <- file.path(filt_path, paste0(sample_names, "_F_no_primer_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample_names, "_R_no_primer_filt.fastq.gz"))

# Check quality after primer cut
	plotQualityProfile(cutFs[1:2])
	plotQualityProfile(cutRs[1:2])

## truncLen=c(forward,reverse) is based on quality result
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs,
                     minLen = 100, # Remove unexpectedly short reads
                     maxN = 0, maxEE = c(2,2), truncQ = 2, rm.phix = T,
                     compress=TRUE, multithread=TRUE)

View(out)

	plotQualityProfile(filtFs[1])
	plotQualityProfile(filtRs[1])

# Exclude 0 seq samples, rename filtFs and filtRs
if(length(sample_names[out[,2]<1 | out[,1]<1]) > 0){
  filtFs <- file.path(filt_path, paste0(sample_names[out[,2]>0 & out[,1]>0], "_F_filt.fastq.gz"))
  filtRs <- file.path(filt_path, paste0(sample_names[out[,2]>0 & out[,1]>0], "_R_filt.fastq.gz"))
}

write.csv(out, paste0(output_folder, "/out_track.csv"))

q()
n

	conda deactivate

###---------------------3 domain pipeline--------------------------------------------
#  Use 3 domain pipeline from https://github.com/jcmcnch/eASV-pipeline-for-515Y-926R
#####---------------------3 domain pipeline--------------------------------------------
#00 cp scripts & 

	cp -r 3_domain_analysis_newbbsplitDB/scripts ./

	cp 3_domain_analysis_newbbsplitDB/515FY-926R.cfg .

- Delete line " ./runscripts/00-denoising-workflow.sh" inside "515FY-926R.cfg" 
- in silico mock "false"

#01 raw data remove primer &filtN (R_DADA2) done

	cp -r 3_domain_analysis_newbbsplitDB/01-trimmed .

	dos2unix sample_name.txt

while read p; 
do
  cp 00_noprimer/"$p"'_06redo_R1.fastq.gz' 01-trimmed/"$p"'.R1.trimmed.fastq.gz';
done < sample_name.txt

while read p; 
do
  cp 00_noprimer/"$p"'_06redo_R2.fastq.gz' 01-trimmed/"$p"'.R2.trimmed.fastq.gz';
done < sample_name.txt

	gunzip 01-trimmed/*.gz

#02 bbsplit

	scripts/01-sort-16S-18S-bbsplit.sh
- files have now been split into 16S and 18S pools, and can be denoised separately

#03 Prokayrotes
	cd 02-PROKs

	cp -r 3_domain_analysis_newbbsplitDB/02-PROKs/scripts ./

	./scripts/P00-create-manifest.sh

	./scripts/P01-import.sh

	./scripts/P02-visualize-quality_R1-R2.sh

	./scripts/P03-DADA2.sh
	#needed for merging script
	./scripts/P04-export-DADA2-results.sh
#-------------------------------Eukaryotes---------------#
#04 Eukaryotes

	cd 3domain_newbbsplitDB/02-EUKs/

	cp -r 3_domain_analysis_newbbsplitDB/02-EUKs/scripts .

	./scripts/E00-create-manifest-viz.sh

	./scripts/E01-import.sh
	#check qc
	./scripts/E02-visualize-quality_R1-R2.sh

	./scripts/E03-bbduk-cut-reads.sh
	#cat tgt
	./scripts/E04-fuse-EUKs-withoutNs.sh

	./scripts/E05-create-manifest-concat.sh

	./scripts/E06-import-concat.sh

	./scripts/E07-visualize-quality-single-seqs.sh

	#do not need trim length since using concatenated fwd+rev reads
	./scripts/E08-DADA2.sh
	./scripts/E09-export-DADA2-results.sh
	
#-----------------------LULUotu for 16s and 18s----------------------#
	cd 3domain_newbbsplitDB/

	mkdir -p 03-LULU

	conda activate R_amplicon

#1 16S
###  blastn

	makeblastdb -in 02-PROKs/03-DADA2d/250201-2257.eDNA.16S.dna-sequences.fasta -parse_seqids -dbtype nucl

	blastn -num_threads 100 -db 02-PROKs/03-DADA2d/250201-2257.eDNA.16S.dna-sequences.fasta -outfmt '6 qseqid sseqid pident' -out 16S_match_list.txt -qcov_hsp_perc 80 -perc_identity 84 -query 02-PROKs/03-DADA2d/250201-2257.eDNA.16S.dna-sequences.fasta

	cp 02-PROKs/03-DADA2d/250201-2257.eDNA.16S.feature-table.biom.tsv ./16S.feature-table.biom.tsv

	sed '1d' 16S.feature-table.biom.tsv > forLULU_16S.feature-table.biom.tsv
	
	R
	library("lulu")

	otutab <- read.csv("forLULU_16S.feature-table.biom.tsv",sep='\t',header=TRUE,as.is=TRUE, row.names = 1)

	row_sums <- rowSums(otutab)

	non_zero_rows <- which(row_sums != 0)

	otutab_filtered <- otutab[non_zero_rows, ]

	matchlist <- read.table("16S_match_list.txt", header=FALSE,as.is=TRUE, stringsAsFactors=FALSE)

	##save result
	b <- data.frame(FilteredZOTU=rownames(otutab_filtered), otutab_filtered)  

	write.table(b, "16s_otutab_filtered.csv", row.names=F, col.names = T, sep=",")

### run LULU

	curated_result <- lulu(otutab_filtered, matchlist)

	# ....Which is equivalent of running LULU with default settings for the options minimum_ratio_type, minimum_ratio, minimum_relative_cooccurence  

	curated_result <- lulu(otutab_filtered, matchlist, minimum_ratio_type = "min", minimum_ratio = 1, minimum_match = 84, minimum_relative_cooccurence = 0.95)

	## Number of OTUs retained  8055
	curated_result$curated_count

	## Number of OTUs discarded 45622
	curated_result$discarded_count

	##save result
	curated_result$curated_table

	a <- data.frame(LULUOTU=rownames(curated_result$curated_table), curated_result$curated_table)  

	write.table(a, "16s_curated_otutab.csv", row.names=F, col.names = T, sep=",")

	write.table(matchlist, "16S_match_list.csv", row.names=F, col.names = F, sep="\t")

	q()
	n

### extract luluotu.fasta

	awk 'BEGIN{ FS=",";OFS="\t" }{ print $1}' 16s_curated_otutab.csv > 16s_luluotuID.txt

	sed -i '1d' 16s_luluotuID.txt

	sed 's/^["]*//g' 16s_luluotuID.txt > 16s_luluotuID2.txt

	sed 's/["]*$//g' 16s_luluotuID2.txt >16s_luluotuID_final.txt

	wc -l 16s_luluotuID_final.txt

	#seqkit extract seqkit按id提取序列
	seqkit grep -f 16s_luluotuID_final.txt 02-PROKs/03-DADA2d/250201-2257.eDNA.16S.dna-sequences.fasta > 16s_luluotu_seq.fa

#---------------------------------------------#
# 2 LULUotu for 3 domain pipeline downstream analysis

	cd 3domain_newbbsplitDB/

	mkdir -p 03-LULU/02-PROKs
	mkdir -p 03-LULU/02-EUKs

	cp -r scripts 03-LULU/

	cp 515FY-926R.cfg 03-LULU/

	cp 02-PROKs/*.tsv 03-LULU/02-PROKs/
	cp 02-EUKs/*.tsv 03-LULU/02-EUKs/

## 16S 
	mkdir -p 03-LULU/02-PROKs/03-DADA2d

	cp -r 02-PROKs/scripts  03-LULU/02-PROKs/

	#import LULUseq & LULUotutab to qiime2
	source 515FY-926R.cfg
	conda activate $qiime2version

	timestamp=`date +"%y%m%d-%H%M"`

	qiime tools import \
	  --type 'FeatureData[Sequence]' \
	  --input-path 16s_luluotu_seq.fa \
	  --output-path 03-LULU/02-PROKs/03-DADA2d/representative_sequences.qza

##Convert OTU table from txt to biom format
	sed '1s/"//g; s/"//g' 16s_curated_otutab.csv | tr ',' '\t' > 16s_luluotutab.txt

	biom convert -i 16s_luluotutab.txt -o 16s_luluotutab.biom \
	  --table-type="OTU table" --to-json

	##Import otu table
	qiime tools import \
	  --input-path 16s_luluotutab.biom \
	  --type 'FeatureTable[Frequency]' \
	  --input-format BIOMV100Format \
	  --output-path 03-LULU/02-PROKs/03-DADA2d/table.qza

##----------------16S after LULU use pipeline----------
	cd 03-LULU/02-PROKs/

	./scripts/P04-export-DADA2-results.sh

	./scripts/P05-classify-eASVs.sh
	./scripts/P07-make-barplot.sh


	./scripts/P09-split-mito-chloro-PR2-reclassify.sh

	#These biom tables are then the input for merging scripts
	./scripts/P10-generate-tsv-biom-tables-with-taxonomy.sh

	#following step optional, gives you proportional data to play with
	./scripts/P11-transform-tsv-to-proportions.sh

	./scripts/P12a-remake-barplot-with-PR2-taxonomy.sh

	./scripts/P12-make-subsetted-barplots.sh


	./scripts/P18-make-asv-tree.sh
###optional 
	#did not cluster this time, seemed to confuse collaborators and adds too much data to CMAP
	#./scripts/P08-optionally-cluster-eASVs.sh
	#./scripts/P13-exclude-samples-from-barplots.sh

	./scripts/P14-optional-reclassify-multiple-p-confidence.sh

	./scripts/P15-optional-new-generate-tsv-biom-tables-with-taxonomy.sh

	./scripts/P16-optional-transform-tsv-to-proportions.sh

	./scripts/P17-optional-merge-taxonomy.sh

##------------18S LULU---------------------#
### 1 blastn
	makeblastdb -in 02-EUKs/08-DADA2d/241022-2032.eDNA.18S.dna-sequences.fasta -parse_seqids -dbtype nucl

	blastn -num_threads 100 -db 02-EUKs/08-DADA2d/241022-2032.eDNA.18S.dna-sequences.fasta -qcov_hsp_perc 80 -perc_identity 84 -query 02-EUKs/08-DADA2d/241022-2032.eDNA.18S.dna-sequences.fasta -outfmt '6 qseqid sseqid pident' -out 18S_match_list.txt

	cp 02-EUKs/08-DADA2d/241022-2032.eDNA.18S.feature-table.biom.tsv ./18S.feature-table.biom.tsv

	sed '1d' 18S.feature-table.biom.tsv > forLULU_18S.feature-table.biom.tsv

	R
	library("lulu")

	otutab <- read.csv("forLULU_18S.feature-table.biom.tsv",sep='\t',header=TRUE,as.is=TRUE, row.names = 1)

	row_sums <- rowSums(otutab)

	non_zero_rows <- which(row_sums != 0)

	otutab_filtered <- otutab[non_zero_rows, ]

	matchlist <- read.table("18S_match_list.txt", header=FALSE,as.is=TRUE, stringsAsFactors=FALSE)

	##save result

	write.table(b, "18s_otutab_filtered.csv", row.names=F, col.names = T, sep=",")

## run LULU 

	curated_result <- lulu(otutab_filtered, matchlist)

	# ....Which is equivalent of running LULU with default settings for the options minimum_ratio_type, minimum_ratio, minimum_relative_cooccurence  

	curated_result <- lulu(otutab_filtered, matchlist, minimum_ratio_type = "min", minimum_ratio = 1, minimum_match = 84, minimum_relative_cooccurence = 0.95)

	## Number of OTUs retained  8055
	curated_result$curated_count

	## Number of OTUs discarded  456
	curated_result$discarded_count

	##save result
	curated_result$curated_table

	a <- data.frame(LULUOTU=rownames(curated_result$curated_table), curated_result$curated_table)  

	write.table(a, "18s_curated_otutab.csv", row.names=F, col.names = T, sep=",")

	q()
	n

## extract luluotu.fasta

	awk 'BEGIN{ FS=",";OFS="\t" }{ print $1}' 18s_curated_otutab.csv > 18s_luluotuID.txt

	sed -i '1d' 18s_luluotuID.txt
	#删除每行开头的"
	sed 's/^["]*//g' 18s_luluotuID.txt > 18s_luluotuID2.txt

	#删除每行结尾的"
	sed 's/["]*$//g' 18s_luluotuID2.txt >18s_luluotuID_final.txt

	wc -l 18s_luluotuID_final.txt

	#seqkit extract seqkit按id提取序列
	seqkit grep -f 18s_luluotuID_final.txt 02-EUKs/08-DADA2d/241022-2032.eDNA.18S.dna-sequences.fasta > 18s_luluotu_seq.fa

#---------------------------------------------#
# LULUotus for 3 domain pipeline downstream analysis

	cd 3_domain_analysis_newbbsplitDB/

	mkdir -p 03-LULU/02-PROKs
	mkdir -p 03-LULU/02-EUKs


	cp -r scripts 03-LULU/

	cp 515FY-926R.cfg 03-LULU/

	cp 02-PROKs/*.tsv 03-LULU/02-PROKs/
	cp 02-EUKs/*.tsv 03-LULU/02-EUKs/

## 16S 
	mkdir -p 03-LULU/02-PROKs/03-DADA2d

	cp -r 02-PROKs/scripts  03-LULU/02-PROKs/

	#import LULUseq & LULUotutab to qiime2
	source 515FY-926R.cfg
	conda activate $qiime2version

	timestamp=`date +"%y%m%d-%H%M"`

	
	qiime tools import \
	  --type 'FeatureData[Sequence]' \
	  --input-path 16s_luluotu_seq.fa \
	  --output-path 03-LULU/02-PROKs/03-DADA2d/representative_sequences.qza

##Convert OTU table from txt to biom format
	sed '1s/"//g; s/"//g' 16s_curated_otutab.csv | tr ',' '\t' > 16s_luluotutab.txt

	biom convert -i 16s_luluotutab.txt -o 16s_luluotutab.biom \
	  --table-type="OTU table" --to-json

	##Import otu table
	qiime tools import \
	  --input-path 16s_luluotutab.biom \
	  --type 'FeatureTable[Frequency]' \
	  --input-format BIOMV100Format \
	  --output-path 03-LULU/02-PROKs/03-DADA2d/table.qza

##----------------16S after LULU use pipeline----------
	cd 03-LULU/02-PROKs/

	./scripts/P04-export-DADA2-results.sh

	./scripts/P05-classify-eASVs.sh
	./scripts/P07-make-barplot.sh


	./scripts/P09-split-mito-chloro-PR2-reclassify.sh

	#These biom tables are then the input for merging scripts
	./scripts/P10-generate-tsv-biom-tables-with-taxonomy.sh

	#following step optional, gives you proportional data to play with
	./scripts/P11-transform-tsv-to-proportions.sh

	./scripts/P12a-remake-barplot-with-PR2-taxonomy.sh

	./scripts/P12-make-subsetted-barplots.sh


	./scripts/P18-make-asv-tree.sh
###optional 
	#did not cluster this time, seemed to confuse collaborators and adds too much data to CMAP
	#./scripts/P08-optionally-cluster-eASVs.sh
	#./scripts/P13-exclude-samples-from-barplots.sh

	./scripts/P14-optional-reclassify-multiple-p-confidence.sh

	./scripts/P15-optional-new-generate-tsv-biom-tables-with-taxonomy.sh

	./scripts/P16-optional-transform-tsv-to-proportions.sh

	./scripts/P17-optional-merge-taxonomy.sh

##------- 18S	
	mkdir -p 03-LULU/02-EUKs/08-DADA2d
	cp -r 02-EUKs/scripts  03-LULU/02-EUKs/

	#import LULUseq & LULUotutab to qiime2 (Use E09-export
	source 515FY-926R.cfg
	conda activate $qiime2version

	timestamp=`date +"%y%m%d-%H%M"`

	qiime tools import \
	  --type 'FeatureData[Sequence]' \
	  --input-path 18s_luluotu_seq.fa \
	  --output-path 03-LULU/02-EUKs/08-DADA2d/representative_sequences.qza

##Convert OTU table from txt to biom format
	sed '1s/"//g; s/"//g' 18s_curated_otutab.csv | tr ',' '\t' > 18s_luluotutab.txt

	biom convert -i 18s_luluotutab.txt -o 18s_luluotutab.biom \
	  --table-type="OTU table" --to-json

	##Import otu table
	qiime tools import \
	  --input-path 18s_luluotutab.biom \
	  --type 'FeatureTable[Frequency]' \
	  --input-format BIOMV100Format \
	  --output-path 03-LULU/02-EUKs/08-DADA2d/table.qza

##----------------18S after LULU use pipeline----------
	cd 03-LULU/02-EUKs/

	./scripts/E09-export-DADA2-results.sh

	./scripts/E10-classify-seqs.sh
	#put sample-metadata files in this directory
	#skip ./scripts/E11-make-sample-metadata-file.sh

	./scripts/E12-make-barplot.sh
	#./scripts/E13-optionally-cluster-eASVs.sh
	./scripts/E14-split-metazoans-PR2-alternative-class.sh
	./scripts/E15-generate-tsv-biom-tables-with-taxonomy.sh
	./scripts/E16-transform-tsv-to-proportions.sh
	./scripts/E23-make-asv-tree.sh

# Summary 16S results
	cd 3_domain_analysis_newbbsplitDB/03-LULU/
	
	nano tax.txt #taxonomy results

	python3 summarizeAbundance.py \
      -i 16S_noMito_noChlo_catLULUOTUtab.tsv \
      -m tax.txt \
      -c '2,3,4,5,6' -s '\t' -n raw \
      -o Prok_16S_afterLULU